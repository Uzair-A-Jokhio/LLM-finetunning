{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import data set\n",
    "\n",
    "    - You can download the Tweet emotion intensity dataset from Hugging Face into your environment.\n",
    "\n",
    "    - Import the file and print out the first few lines of it.\n",
    "\n",
    "The following code snippet will help you load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/LLM-finetunning/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "      <th>class_intensity</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40815</td>\n",
       "      <td>Loved @Bethenny independence msg on @WendyWill...</td>\n",
       "      <td>fear</td>\n",
       "      <td>low</td>\n",
       "      <td>fear_low</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10128</td>\n",
       "      <td>@mark_slifer actually maybe we were supposed t...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>high</td>\n",
       "      <td>sadness_high</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40476</td>\n",
       "      <td>I thought the nausea and headaches had passed ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>medium</td>\n",
       "      <td>fear_medium</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20813</td>\n",
       "      <td>Anger, resentment, and hatred are the destroye...</td>\n",
       "      <td>anger</td>\n",
       "      <td>high</td>\n",
       "      <td>anger_high</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40796</td>\n",
       "      <td>new tires &amp;amp; an alarm system on my car. fwm...</td>\n",
       "      <td>fear</td>\n",
       "      <td>low</td>\n",
       "      <td>fear_low</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet    class  \\\n",
       "0  40815  Loved @Bethenny independence msg on @WendyWill...     fear   \n",
       "1  10128  @mark_slifer actually maybe we were supposed t...  sadness   \n",
       "2  40476  I thought the nausea and headaches had passed ...     fear   \n",
       "3  20813  Anger, resentment, and hatred are the destroye...    anger   \n",
       "4  40796  new tires &amp; an alarm system on my car. fwm...     fear   \n",
       "\n",
       "  sentiment_intensity class_intensity  labels  \n",
       "0                 low        fear_low       4  \n",
       "1                high    sadness_high       9  \n",
       "2              medium     fear_medium       5  \n",
       "3                high      anger_high       0  \n",
       "4                 low        fear_low       4  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "data = pd.read_csv(\"hf://datasets/stepp1/tweet_emotion_intensity/\" + splits[\"train\"])\n",
    "test_data = pd.read_csv(\"hf://datasets/stepp1/tweet_emotion_intensity/\" + splits[\"test\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2: clean data:\n",
    "This step is cleaning the raw text data to remove unnecessary characters, such as URLs, special symbols, or HTML tags, and to normalize the text by converting it to lowercase. \n",
    "\n",
    "Make a new column called cleanedText that is equal to the data in the Tweet column that has had this cleanedText function applied to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "      <th>class_intensity</th>\n",
       "      <th>labels</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40815</td>\n",
       "      <td>Loved @Bethenny independence msg on @WendyWill...</td>\n",
       "      <td>fear</td>\n",
       "      <td>low</td>\n",
       "      <td>fear_low</td>\n",
       "      <td>4</td>\n",
       "      <td>loved bethenny independence msg on wendywillia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10128</td>\n",
       "      <td>@mark_slifer actually maybe we were supposed t...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>high</td>\n",
       "      <td>sadness_high</td>\n",
       "      <td>9</td>\n",
       "      <td>mark_slifer actually maybe we were supposed to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40476</td>\n",
       "      <td>I thought the nausea and headaches had passed ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>medium</td>\n",
       "      <td>fear_medium</td>\n",
       "      <td>5</td>\n",
       "      <td>i thought the nausea and headaches had passed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20813</td>\n",
       "      <td>Anger, resentment, and hatred are the destroye...</td>\n",
       "      <td>anger</td>\n",
       "      <td>high</td>\n",
       "      <td>anger_high</td>\n",
       "      <td>0</td>\n",
       "      <td>anger resentment and hatred are the destroyer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40796</td>\n",
       "      <td>new tires &amp;amp; an alarm system on my car. fwm...</td>\n",
       "      <td>fear</td>\n",
       "      <td>low</td>\n",
       "      <td>fear_low</td>\n",
       "      <td>4</td>\n",
       "      <td>new tires amp an alarm system on my car fwm now</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet    class  \\\n",
       "0  40815  Loved @Bethenny independence msg on @WendyWill...     fear   \n",
       "1  10128  @mark_slifer actually maybe we were supposed t...  sadness   \n",
       "2  40476  I thought the nausea and headaches had passed ...     fear   \n",
       "3  20813  Anger, resentment, and hatred are the destroye...    anger   \n",
       "4  40796  new tires &amp; an alarm system on my car. fwm...     fear   \n",
       "\n",
       "  sentiment_intensity class_intensity  labels  \\\n",
       "0                 low        fear_low       4   \n",
       "1                high    sadness_high       9   \n",
       "2              medium     fear_medium       5   \n",
       "3                high      anger_high       0   \n",
       "4                 low        fear_low       4   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  loved bethenny independence msg on wendywillia...  \n",
       "1  mark_slifer actually maybe we were supposed to...  \n",
       "2  i thought the nausea and headaches had passed ...  \n",
       "3  anger resentment and hatred are the destroyer ...  \n",
       "4    new tires amp an alarm system on my car fwm now  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_data(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text) # Remove URLs from the text\n",
    "    text = re.sub(r\"<.*?>\", '', text) # Remove any HTML tags from the text\n",
    "    text = re.sub(r\"[^\\w\\s]\", '',text) # Remove punctuation, keep only words and spaces\n",
    "    return text\n",
    "\n",
    "data['cleaned_text'] = data['tweet'].apply(clean_data)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 3: handle missing data\n",
    "We now handle missing or incomplete data in your dataset. You can either remove rows with missing data or fill them with placeholders, ensuring the dataset is complete for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                     0\n",
      "tweet                  0\n",
      "class                  0\n",
      "sentiment_intensity    0\n",
      "class_intensity        0\n",
      "labels                 0\n",
      "cleaned_text           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 4: tokenizer\n",
    "After cleaning the text, we tokenize it. Tokenization splits the text into individual words or subwords that can be used by the model. We will use the BERT tokenizer to ensure compatibility with the Brie-trained model you are fine-tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  3866,  7014,  2368,  4890,  4336,  5796,  2290,  2006, 12815,\n",
      "         29602,  6632,  5244,  2022,  3407, 23713, 16829,  2306,  4426, 23713,\n",
      "         13433, 28032,  7730,  2097, 19311,  2000,  2017,  3407,  2981,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  2928,  1035, 22889, 23780,  2941,  2672,  2057,  2020,  4011,\n",
      "          2000,  3280,  1998,  2026, 13445,  5552,  2256,  3268, 27451,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  1045,  2245,  1996, 19029,  1998, 14978,  2015,  2018,  2979,\n",
      "          2021,  8840,  2140,  1045,  2514,  9643,  2651,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  4963, 20234,  1998, 11150,  2024,  1996,  9799,  1997,  2115,\n",
      "          7280,  2651,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  2047, 13310, 23713,  2019,  8598,  2291,  2006,  2026,  2482,\n",
      "          1042,  2860,  2213,  2085,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "token = tokenizer(\n",
    "    data['cleaned_text'].tolist(), \n",
    "    padding=True,\n",
    "    truncation=True, \n",
    "    max_length=128, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(token['input_ids'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the BERT tokenizer from the Transformers library from HuggingFace, and tokenize the cleanedText that we defined earlier. Then, we can print the tokens of the input IDs. The words have been converted into these numbered tokens. \n",
    "\n",
    "In certain cases, especially when data is limited, data augmentation techniques can be applied to generate new training examples by modifying the original dataset.\n",
    "\n",
    "    Paraphrasing: rewriting sentences in different ways while preserving the meaning\n",
    "\n",
    "    Backtranslation: translating text into another language and back again to create variation\n",
    "\n",
    "    Synonym replacement: replacing certain words in the text with their synonyms\n",
    "\n",
    "## Augmentation\n",
    "\n",
    "The following example demonstrates how to implement synonym replacement using the nltk library. It randomly replaces words in the text with their synonyms to create new variations of sentences. This method can be applied when paraphrasing or backtranslation is not feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import random # Random module for generating random numbers and selections\n",
    "import nltk # NLTK's WordNet corpus for finding synonyms\n",
    "nltk.download('wordnet')\n",
    "# Define a function to find and replace a word with a synonym\n",
    "def synonym_replacement(word):\n",
    "# Get all synsets (sets of synonyms) for the given word from WordNet\n",
    "    synonyms = wordnet.synsets(word)\n",
    "\n",
    "# If the word has synonyms, randomly choose one synonym, otherwise return the original word\n",
    "    if synonyms:\n",
    "# Select a random synonym and get the first lemma (word form) of that synonym\n",
    "        return random.choice(synonyms).lemmas()[0].name()\n",
    "\n",
    "# If no synonyms are found, return the original word\n",
    "    return word\n",
    "\n",
    "# Define a function to augment text by replacing words with synonyms randomly\n",
    "def augment_text(text):\n",
    "# Split the input text into individual words\n",
    "    words = text.split() # Split the input text into individual words\n",
    "\n",
    "# Replace each word with a synonym with a probability of 20% (random.random() > 0.8)\n",
    "    augmented_words = [\n",
    "    synonym_replacement(word) if random.random() > 0.8 else word \n",
    "# If random condition met, replace\n",
    "for word in words] # Iterate over each word in the original text\n",
    "\n",
    "# Join the augmented words back into a single string and return it\n",
    "    return ' '.join(augmented_words)\n",
    "\n",
    "# Apply the text augmentation function to the 'cleaned_text' column in a DataFrame\n",
    "# Create a new column 'augmented_text' containing the augmented version of 'cleaned_text'\n",
    "data['augmented_text'] = data['cleaned_text'].apply(augment_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 5: structure of fine tunning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "input_ids = token['input_ids']\n",
    "attention_mask = token['attention_mask'] \n",
    "\n",
    "def map_sentiment(value):\n",
    "    if value == \"high\":\n",
    "        return 1\n",
    "    elif value == \"medium\":\n",
    "        return 0.5\n",
    "    elif value == \"low\":\n",
    "        return 0\n",
    "    else:\n",
    "        return None  # Handle unexpected values, if any\n",
    "\n",
    "# Apply the function to each item in 'sentiment_intensity'\n",
    "data['sentiment_intensity'] = data['sentiment_intensity'].apply(map_sentiment)\n",
    "\n",
    "# Drop any rows where 'sentiment_intensity' is None\n",
    "data = data.dropna(subset=['sentiment_intensity']).reset_index(drop=True)\n",
    "\n",
    "# Convert the 'sentiment_intensity' column to a tensor\n",
    "labels = torch.tensor(data['sentiment_intensity'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 6: split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, validation, and test sets are prepared with attention masks!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split # Import function to split dataset\n",
    "\n",
    "# First split: 15% for test set, the rest for training/validation\n",
    "train_val_inputs, test_inputs, train_val_masks, test_masks, train_val_labels, test_labels = train_test_split(\n",
    "    input_ids, attention_mask, labels, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 20% for validation set from remaining data\n",
    "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    train_val_inputs, train_val_masks, train_val_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create TensorDataset objects for each set, including attention masks\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "print(\"Training, validation, and test sets are prepared with attention masks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
